聚类使用算法：word2vec，词向量

word2vec

## 词向量

密集向量：通过训练，将词汇映射一个固定维度的向量（该向量远小于词典大小）



**独热编码** | one-hot representation

稀疏向量：对于一个词典，使用词典的大小表示一个词的维数；词典里面有多少个词汇就有多少个维度。每个词汇的向量表示在自己的索引位置上为1、其余为0

稀疏向量的词集表示和词袋表示。**词集表示**表示记录文字是否出现，出现为1、反之为0；**词集表示**记录词汇出现的次数，将所有的词向量相加



**TF-IDF|bm25**

TF是词频，表示词条在某一个文本中的出现频率

TF = 某个词在文档中出现的次数/文档长度

> 某文档D，长度为200，其中“Bert”出现了2次，“的”出现了20次，“原理”出现了3次，那么:
> TF(Bert|D) = 2/200 = 0.01
> TF(的|D) = 20/200 = 0.1
> TF(原理|D) = 3/200 = 0.015
>
> “Bert的原理”这个短语与文档D的相关性就是三个词的相关性之和。
> TF(Bert的原理|D) = 0.01 + 0.1 + 0.015 = 0.125

IDF是逆文档频率

IDF（词语） = log（语料库的文档总数/（包含词语的文档总数+1））

> N表示全部文档数。假如世界上文档总数位100亿，"Bert"在1万个文档中出现过，“原理”在2亿个文档中出现过，那么它们的IDF值分别为：
> IDF(Bert) = log(100亿/1万) = 19.93
> IDF(原理) ＝ log(100亿/2亿) ＝ 5.64
>
> “Bert”重要性相当于“原理”的3.5倍。停用词“的”在所有的文档里出现过，它的IDF=log(1)=0。

短语和文档的最终相关由TF和IDF加权求和

simlarity = TF1\*IDF1 + TF2\*IDF2 + ... + TFn\*IDFn



**修正1:使用对数降低TF的无线增长**

BM25的 TF Score = ((k + 1) * tf) / (k + tf)

使得TF不会无线增长,当到达一个阈值之后TF的区分度下降

它可以无限逼近k+1,但是永远无法触达它

**修正2:文档的长度会影响词的词频**

BM25还引入了平均文档长度的概念，BM25的TF公式里，除了k外，引入另外两个参数：L和b。L是文档长度与平均长度的比值，b是一个常数，它的作用是规定L对评分的影响有多大。
公式：TF Score = ((k + 1) * tf) / (k * (1.0 - b + b * L) + tf)

文档越短，它逼近上限的速度越快。
对于只有几个词的内容，比如文章“标题”，只需要匹配很少
的几个词，就可以确定相关性。而对于大篇幅的内容，比如一
本书的内容，需要匹配很多词才能知道它的重点是讲什么

最终:
simlarity = IDF * ((k + 1) * tf) / (k * (1.0 - b + b * (|d|/avgDl)) + tf)